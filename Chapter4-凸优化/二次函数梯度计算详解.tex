\documentclass[12pt,a4paper]{article}
\usepackage[UTF8]{ctex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

\title{二次函数梯度计算详解}
\subtitle{为什么 $\nabla f_0(\mathbf{x}) = \mathbf{P}\mathbf{x} + \mathbf{q}$？}
\author{}
\date{\today}

\begin{document}

\maketitle

\section{问题提出}

对于二次函数：

\begin{equation}
f_0(\mathbf{x}) = \frac{1}{2}\mathbf{x}^T \mathbf{P} \mathbf{x} + \mathbf{q}^T \mathbf{x} + r
\end{equation}

\textbf{问题}：为什么梯度是 $\nabla f_0(\mathbf{x}) = \mathbf{P}\mathbf{x} + \mathbf{q}$？

\section{梯度的定义}

\subsection{标量函数的梯度}

\textbf{梯度}：对于函数 $f : \mathbb{R}^n \to \mathbb{R}$，梯度定义为：

\begin{equation}
\nabla f(\mathbf{x}) = \begin{pmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{pmatrix}
\end{equation}

\textbf{含义}：梯度是偏导数组成的向量，指向函数值增加最快的方向。

\section{二次函数的组成}

\subsection{函数分解}

二次函数 $f_0(\mathbf{x}) = \frac{1}{2}\mathbf{x}^T \mathbf{P} \mathbf{x} + \mathbf{q}^T \mathbf{x} + r$ 由三部分组成：

\begin{enumerate}
\item \textbf{二次项}：$\frac{1}{2}\mathbf{x}^T \mathbf{P} \mathbf{x}$

\item \textbf{线性项}：$\mathbf{q}^T \mathbf{x}$

\item \textbf{常数项}：$r$
\end{enumerate}

\textbf{梯度计算}：由于梯度是线性算子，有：

\begin{equation}
\nabla f_0(\mathbf{x}) = \nabla\left(\frac{1}{2}\mathbf{x}^T \mathbf{P} \mathbf{x}\right) + \nabla(\mathbf{q}^T \mathbf{x}) + \nabla r
\end{equation}

\section{各部分梯度的计算}

\subsection{常数项的梯度}

\textbf{常数项}：$r$（常数）

\textbf{梯度}：

\begin{equation}
\nabla r = \mathbf{0}
\end{equation}

\textbf{原因}：常数对任何变量的偏导数都是 0。

\subsection{线性项的梯度}

\textbf{线性项}：$\mathbf{q}^T \mathbf{x} = q_1 x_1 + q_2 x_2 + \cdots + q_n x_n$

\textbf{计算梯度}：

\begin{align}
\frac{\partial}{\partial x_i} (\mathbf{q}^T \mathbf{x}) &= \frac{\partial}{\partial x_i} (q_1 x_1 + q_2 x_2 + \cdots + q_n x_n) \\
&= q_i
\end{align}

\textbf{因此}：

\begin{equation}
\nabla (\mathbf{q}^T \mathbf{x}) = \begin{pmatrix}
q_1 \\
q_2 \\
\vdots \\
q_n
\end{pmatrix} = \mathbf{q}
\end{equation}

\subsection{二次项的梯度}

\textbf{二次项}：$\frac{1}{2}\mathbf{x}^T \mathbf{P} \mathbf{x}$

\textbf{展开形式}：

设 $\mathbf{P} = [p_{ij}]$，$\mathbf{x} = (x_1, x_2, \ldots, x_n)^T$，则：

\begin{align}
\mathbf{x}^T \mathbf{P} \mathbf{x} &= \sum_{i=1}^n \sum_{j=1}^n p_{ij} x_i x_j \\
&= \sum_{i=1}^n p_{ii} x_i^2 + 2\sum_{i < j} p_{ij} x_i x_j
\end{align}

\textbf{方法1：直接计算偏导数}

对 $x_k$ 求偏导数：

\begin{align}
\frac{\partial}{\partial x_k} (\mathbf{x}^T \mathbf{P} \mathbf{x}) &= \frac{\partial}{\partial x_k} \left(\sum_{i=1}^n \sum_{j=1}^n p_{ij} x_i x_j\right) \\
&= \sum_{i=1}^n \sum_{j=1}^n p_{ij} \frac{\partial}{\partial x_k}(x_i x_j) \\
&= \sum_{i=1}^n p_{ik} x_i + \sum_{j=1}^n p_{kj} x_j \\
&= \sum_{i=1}^n p_{ik} x_i + \sum_{i=1}^n p_{ki} x_i \\
&= \sum_{i=1}^n (p_{ik} + p_{ki}) x_i
\end{align}

\textbf{如果 $\mathbf{P}$ 是对称矩阵}（$p_{ik} = p_{ki}$）：

\begin{equation}
\frac{\partial}{\partial x_k} (\mathbf{x}^T \mathbf{P} \mathbf{x}) = 2 \sum_{i=1}^n p_{ki} x_i = 2[\mathbf{P}\mathbf{x}]_k
\end{equation}

\textbf{因此}：

\begin{equation}
\nabla (\mathbf{x}^T \mathbf{P} \mathbf{x}) = 2\mathbf{P}\mathbf{x}
\end{equation}

\textbf{对于 $\frac{1}{2}\mathbf{x}^T \mathbf{P} \mathbf{x}$}：

\begin{equation}
\nabla\left(\frac{1}{2}\mathbf{x}^T \mathbf{P} \mathbf{x}\right) = \frac{1}{2} \cdot 2\mathbf{P}\mathbf{x} = \mathbf{P}\mathbf{x}
\end{equation}

\textbf{方法2：使用矩阵微积分}

\textbf{公式}：对于二次型 $\mathbf{x}^T \mathbf{A} \mathbf{x}$，如果 $\mathbf{A}$ 是对称矩阵，则：

\begin{equation}
\nabla (\mathbf{x}^T \mathbf{A} \mathbf{x}) = 2\mathbf{A}\mathbf{x}
\end{equation}

\textbf{证明思路}：

考虑函数 $g(\mathbf{x}) = \mathbf{x}^T \mathbf{A} \mathbf{x}$，对于小的 $\mathbf{h}$，有：

\begin{align}
g(\mathbf{x} + \mathbf{h}) &= (\mathbf{x} + \mathbf{h})^T \mathbf{A} (\mathbf{x} + \mathbf{h}) \\
&= \mathbf{x}^T \mathbf{A} \mathbf{x} + \mathbf{x}^T \mathbf{A} \mathbf{h} + \mathbf{h}^T \mathbf{A} \mathbf{x} + \mathbf{h}^T \mathbf{A} \mathbf{h} \\
&= \mathbf{x}^T \mathbf{A} \mathbf{x} + (\mathbf{A}^T \mathbf{x})^T \mathbf{h} + (\mathbf{A}\mathbf{x})^T \mathbf{h} + O(\|\mathbf{h}\|^2) \\
&= \mathbf{x}^T \mathbf{A} \mathbf{x} + (\mathbf{A}^T \mathbf{x} + \mathbf{A}\mathbf{x})^T \mathbf{h} + O(\|\mathbf{h}\|^2)
\end{align}

如果 $\mathbf{A}$ 是对称矩阵（$\mathbf{A}^T = \mathbf{A}$），则：

\begin{equation}
g(\mathbf{x} + \mathbf{h}) = \mathbf{x}^T \mathbf{A} \mathbf{x} + 2(\mathbf{A}\mathbf{x})^T \mathbf{h} + O(\|\mathbf{h}\|^2)
\end{equation}

因此 $\nabla g(\mathbf{x}) = 2\mathbf{A}\mathbf{x}$。

\section{完整的梯度计算}

\subsection{组合结果}

\textbf{各部分梯度}：
\begin{itemize}
\item $\nabla\left(\frac{1}{2}\mathbf{x}^T \mathbf{P} \mathbf{x}\right) = \mathbf{P}\mathbf{x}$
\item $\nabla(\mathbf{q}^T \mathbf{x}) = \mathbf{q}$
\item $\nabla r = \mathbf{0}$
\end{itemize}

\textbf{总梯度}：

\begin{equation}
\nabla f_0(\mathbf{x}) = \mathbf{P}\mathbf{x} + \mathbf{q} + \mathbf{0} = \mathbf{P}\mathbf{x} + \mathbf{q}
\end{equation}

\section{具体例子}

\subsection{例子1：二维情况}

\textbf{函数}：$f_0(x, y) = \frac{1}{2}(x, y) \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} + (1, 2) \begin{pmatrix} x \\ y \end{pmatrix} + 3$

\textbf{展开}：

\begin{align}
f_0(x, y) &= \frac{1}{2}(2x^2 + 2xy + 2y^2) + x + 2y + 3 \\
&= x^2 + xy + y^2 + x + 2y + 3
\end{align}

\textbf{计算梯度}：

\begin{align}
\frac{\partial f_0}{\partial x} &= 2x + y + 1 \\
\frac{\partial f_0}{\partial y} &= x + 2y + 2
\end{align}

\textbf{矩阵形式}：

\begin{equation}
\nabla f_0 = \begin{pmatrix} 2x + y + 1 \\ x + 2y + 2 \end{pmatrix} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} + \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \mathbf{P}\mathbf{x} + \mathbf{q} \quad \checkmark
\end{equation}

\subsection{例子2：简单情况}

\textbf{函数}：$f_0(x, y) = x^2 + y^2 + x + y$

\textbf{矩阵形式}：

\begin{equation}
f_0(\mathbf{x}) = \frac{1}{2}\mathbf{x}^T \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix} \mathbf{x} + (1, 1) \mathbf{x}
\end{equation}

\textbf{梯度}：

\begin{equation}
\nabla f_0 = \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} + \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2x + 1 \\ 2y + 1 \end{pmatrix}
\end{equation}

\textbf{验证}：

\begin{align}
\frac{\partial f_0}{\partial x} &= 2x + 1 \quad \checkmark \\
\frac{\partial f_0}{\partial y} &= 2y + 1 \quad \checkmark
\end{align}

\section{矩阵微积分规则}

\subsection{常用公式}

\begin{enumerate}
\item \textbf{线性函数}：$\nabla (\mathbf{a}^T \mathbf{x}) = \mathbf{a}$

\item \textbf{二次型}：$\nabla (\mathbf{x}^T \mathbf{A} \mathbf{x}) = (\mathbf{A} + \mathbf{A}^T)\mathbf{x}$

\item \textbf{对称矩阵的二次型}：如果 $\mathbf{A} = \mathbf{A}^T$，则 $\nabla (\mathbf{x}^T \mathbf{A} \mathbf{x}) = 2\mathbf{A}\mathbf{x}$

\item \textbf{常数}：$\nabla c = \mathbf{0}$
\end{enumerate}

\subsection{线性性质}

\textbf{线性性质}：梯度是线性算子

\begin{equation}
\nabla (\alpha f + \beta g) = \alpha \nabla f + \beta \nabla g
\end{equation}

这解释了为什么可以分别计算各部分再相加。

\section{为什么需要 $\frac{1}{2}$？}

\subsection{原因}

\textbf{问题}：为什么二次函数写成 $\frac{1}{2}\mathbf{x}^T \mathbf{P} \mathbf{x}$ 而不是 $\mathbf{x}^T \mathbf{P} \mathbf{x}$？

\textbf{原因}：为了简化梯度表达式。

\textbf{对比}：
\begin{itemize}
\item 如果 $f_0(\mathbf{x}) = \mathbf{x}^T \mathbf{P} \mathbf{x} + \mathbf{q}^T \mathbf{x} + r$，则 $\nabla f_0(\mathbf{x}) = 2\mathbf{P}\mathbf{x} + \mathbf{q}$
\item 如果 $f_0(\mathbf{x}) = \frac{1}{2}\mathbf{x}^T \mathbf{P} \mathbf{x} + \mathbf{q}^T \mathbf{x} + r$，则 $\nabla f_0(\mathbf{x}) = \mathbf{P}\mathbf{x} + \mathbf{q}$
\end{itemize}

\textbf{优势}：
\begin{itemize}
\item 梯度表达式更简洁（没有系数 2）
\item 与Hessian矩阵一致：$\nabla^2 f_0(\mathbf{x}) = \mathbf{P}$
\end{itemize}

\section{总结}

\subsection{梯度计算步骤}

\begin{enumerate}
\item \textbf{分解函数}：将函数分解为二次项、线性项、常数项

\item \textbf{分别计算}：
   \begin{itemize}
   \item $\nabla\left(\frac{1}{2}\mathbf{x}^T \mathbf{P} \mathbf{x}\right) = \mathbf{P}\mathbf{x}$
   \item $\nabla(\mathbf{q}^T \mathbf{x}) = \mathbf{q}$
   \item $\nabla r = \mathbf{0}$
   \end{itemize}

\item \textbf{组合结果}：$\nabla f_0(\mathbf{x}) = \mathbf{P}\mathbf{x} + \mathbf{q}$
\end{enumerate}

\subsection{关键公式}

\begin{enumerate}
\item \textbf{二次型梯度}：$\nabla (\mathbf{x}^T \mathbf{A} \mathbf{x}) = (\mathbf{A} + \mathbf{A}^T)\mathbf{x}$

\item \textbf{对称矩阵}：如果 $\mathbf{A} = \mathbf{A}^T$，则 $\nabla (\mathbf{x}^T \mathbf{A} \mathbf{x}) = 2\mathbf{A}\mathbf{x}$

\item \textbf{带系数}：$\nabla\left(\frac{1}{2}\mathbf{x}^T \mathbf{P} \mathbf{x}\right) = \mathbf{P}\mathbf{x}$（当 $\mathbf{P}$ 对称时）
</enumerate}

\subsection{关键理解}

\begin{enumerate}
\item \textbf{梯度是线性算子}：可以分别计算各部分

\item \textbf{二次型求导}：需要特别注意矩阵的对称性

\item \textbf{系数 $\frac{1}{2}$}：简化梯度表达式，使其与Hessian矩阵一致
</enumerate}

理解梯度计算，是掌握优化理论的基础！

\end{document}

